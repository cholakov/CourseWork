{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('TkAgg')  # avoid non-GUI warning for matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from itertools import product as cartesian_product\n",
    "from skimage.draw import circle, circle_perimeter\n",
    "\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(linewidth=250,precision=2)\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/37370015/what-is-the-difference-between-value-iteration-and-policy-iteration\n",
    "# sutton terminal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeGenerator(object):\n",
    "    def __init__(self):\n",
    "        self.maze = None\n",
    "    \n",
    "    def init_end_states(self):\n",
    "        \"\"\"Get start and end goals\"\"\"\n",
    "        init_state = [3,1]\n",
    "        goal_states = [[11,17]]\n",
    "        return init_state, goal_states\n",
    "        \n",
    "    def get_maze(self):\n",
    "        return self.maze\n",
    "\n",
    "\n",
    "class SimpleMazeGenerator(MazeGenerator):\n",
    "    def __init__(self, maze):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.maze = maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Region ID's (also related to the region colours, more in utils.py - line 113)\n",
    "regions = {'water':6, 'edge':7, 'sky':8, 'mine':4, 'uncharted':5}\n",
    "\n",
    "env_size = 19\n",
    "maze_array = np.zeros([env_size,env_size]) + regions['water']\n",
    "\n",
    "#Boundary\n",
    "maze_array[:,0] = regions['edge']\n",
    "maze_array[:,-1] = regions['edge']\n",
    "maze_array[-1,:] = regions['edge']\n",
    "maze_array[0:2,:] = regions['sky']\n",
    "\n",
    "#Bombs\n",
    "maze_array[circle(11,3,2)] = regions['mine']\n",
    "maze_array[circle(6,14,2)] = regions['mine']\n",
    "maze_array[circle(15,4,2)] = regions['mine']\n",
    "maze_array[circle(5,10,2)] = regions['mine']\n",
    "maze_array[circle(13,16,2)] = regions['mine']\n",
    "maze_array[circle(4,4,2)] = regions['mine']\n",
    "\n",
    "#Unmapped territory\n",
    "maze_array[circle(12,10,5)] = regions['uncharted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Generate the underwater maze environment\n",
    "maze = SimpleMazeGenerator(maze_array)\n",
    "submarine = MazeEnv(maze, render_trace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10e46fcf8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIWJgm/QEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b66a908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A view of the environment\n",
    "\n",
    "#reset(), resets the environment back to it's initial conditions and returns a view of it\n",
    "snapshot = submarine._reset() \n",
    "\n",
    "plt.imshow(snapshot, cmap=submarine.cmap, norm=submarine.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of Actions :  4 \n",
      " Action ID: Up, Down, Left, Right =  [0, 1, 2, 3] \n",
      " Goal states:  [[11, 17]] \n",
      " Maze size:  (19, 19) \n",
      " List of all states in the environment:  [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4)] ...\n"
     ]
    }
   ],
   "source": [
    "# Some available information from the environment\n",
    "print(  \" Number of Actions : \", submarine.num_actions,\n",
    "      \"\\n Action ID: Up, Down, Left, Right = \",submarine.all_actions,\n",
    "      \"\\n Goal states: \", submarine.goal_states,\n",
    "      \"\\n Maze size: \", submarine.maze_size,\n",
    "      \"\\n List of all states in the environment: \", submarine.valid_states[0:5], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1714512d30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYmQ+Kd0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b66a748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unlike the traditional gym environment, each call to the step() function executes an \n",
    "# action and takes a step through the dynamics of the environment.\n",
    "# The resulting environmental observation is returned. \n",
    "# Note: This function updates the state of the executing agent.\n",
    "\n",
    "\n",
    "submarine._step(1)\n",
    "submarine._step(1)\n",
    "submarine._step(1)\n",
    "snapshot = submarine._step(1)\n",
    "plt.imshow(snapshot, cmap=submarine.cmap, norm=submarine.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1714994c18>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYvj9IecAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17145018d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submarine._step(3)\n",
    "submarine._step(3)\n",
    "snapshot = submarine._step(3)\n",
    "plt.imshow(snapshot, cmap=submarine.cmap, norm=submarine.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _next_state(current_state,action) takes a state and an action as an input and \n",
    "# returns the resulting state.\n",
    "# Note: This function does not update the state of the executing agent.\n",
    "submarine._next_state((0,0),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CourseWork\n",
    "\n",
    "You are to design an agent that will be in charge of the navigation of a submarine, through a\n",
    "minefield, towards a goal.\n",
    "\n",
    "Navigating through uncharted territory can be dangerous, which we will model by applying some damage to the submarine. Additionally, there is some uncertainty in the execution of actions when moving through uncharted territory. Currents are strong and there is a 20% chance that the submarine will diverge away from its path during execution. For example, given that our current state is (12,10) within the uncharted territory, and we execute a down action; there is an 80% chance it will go down to (13,10), but also a 10% chance that it will go left to (12,9), and another 10% chance it will go right to (12,11).\n",
    "\n",
    "\n",
    "Hitting a mine will lead to the destruction of the submarine and boundary regions marked **edge** or **sky** cannot be traversed.  Your task is to develop a strategy for this agent (plotted as the *blue* dot) to safely navigate towards the goal (plotted as the *green* dot).\n",
    "\n",
    "You are to do this using the principles of Dynamic Programming. In particular, in this exercise,\n",
    "you will implement the methods of Policy and Value Iteration. For all of these questions, please use\n",
    "the provided visualisation functions to plot your policies and their associated trajectories. Include\n",
    "concise comments within the notebook to highlight any interesting observations to support your\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 : Reward and Transition Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define suitable reward and transition functions for the submarine environment. This is a key task for you as the\n",
    "designer of this agent. \n",
    "\n",
    "The reward function specifies a scalar reward for every state in the environment (in accordance to the coursework description above) and the transition function determines the next *probable* state, $s'$, reached when an action $a$ is taken from state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Writeup: Reasoning and Assumptions**\n",
    "\n",
    "In writing the Transition and the Reward functions, I am making the decision to assign the same amount of reward the movements inside unchartered waters as in normal waters. I will model the uncertainty of being in unchartered waters through probabilistic transitions – the actions of the submarine crew do not always yield the expected results and the vessel occasionally diverges due to strong currents. Of course, in real world scenarios, the unchartered territory can hide extra dangers. If that's the case get_reward can be easily modified to assign numeric damage to simulations inside the unchartered areas.\n",
    "\n",
    "The reason why I am deciding not to assign explicit reward damage is because I want to obsereve the effect of probabilistic tranisitions. My expectation is that the submarine should learn to avoid squares which are 1. unchartered, and 2. in the viscinity of mines because the submarine can be sent onto the mine and there is nothing the crew can do about that. Let's see if that's what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2341)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "regions = {'water':6, 'edge':7, 'sky':8, 'mine':4, 'uncharted':5}\n",
    "maze_array[y,x]\n",
    "Action ID: Up, Down, Left, Right =  [0, 1, 2, 3] \n",
    "\n",
    "0 Up\n",
    "1 Down\n",
    "2 Left\n",
    "3 Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounds_check(state, direction): \n",
    "    \"\"\"\n",
    "    This is a helper function used by get_transition.\n",
    "    \n",
    "    Given a direction of movement, this method always returns the transition state \n",
    "    as determined by _next_state(), unless the result falls outside the \n",
    "    bounds of the environment. In that case, the method returns the same state.\n",
    "\n",
    "    \"\"\"\n",
    "    goalState = submarine._next_state(state, direction)\n",
    "    if \\\n",
    "    (goalState[0] < 0) or \\\n",
    "    (goalState[1] < 0) or \\\n",
    "    (goalState[0] > env_size - 1) or \\\n",
    "    (goalState[1] > env_size - 1):\n",
    "        return state\n",
    "    else:\n",
    "        return goalState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition(currState, action, probabilistic=False):\n",
    "    \"\"\"\n",
    "    The Transition Function returns the next state given the \n",
    "    current state and an action.\n",
    "    \n",
    "    Transitions can be either probabilistic or deterministic. In general,\n",
    "    we always want to set 'probabilistic = True' when we do Value Iteration \n",
    "    and Policy Iteration to model the uncertainty when navigating through \n",
    "    unchartered waters. 'probabilistic = False' can be useful sometimes, \n",
    "    for example, to extract one best policy after we're done with Value \n",
    "    Iteration (Q 1.2).\n",
    "    \n",
    "    Note that if currState is absorbing (a mine or the goal), we transition \n",
    "    to the same state in perpetuity.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Check type of currState (mine, water, etc..)\n",
    "    currRegion = maze_array[currState[0]][currState[1]] \n",
    "    \n",
    "    # Check if currState is absorbing\n",
    "    absorbing = False\n",
    "    if currRegion == 4 or currState in submarine.goal_states:\n",
    "        absorbing = True\n",
    "    \n",
    "    if probabilistic:\n",
    "        if absorbing:\n",
    "            return [(currState, 1.0)]\n",
    "        possible_next_states = []\n",
    "        if currRegion == 5: # unchartered waters\n",
    "            possible_next_states.append((bounds_check(currState, action), 0.8))\n",
    "            if action in (2,3):\n",
    "                possible_next_states.append((bounds_check(currState, 0), 0.1))\n",
    "                possible_next_states.append((bounds_check(currState, 1), 0.1))\n",
    "            elif action in (0,1):\n",
    "                possible_next_states.append((bounds_check(currState, 2), 0.1))\n",
    "                possible_next_states.append((bounds_check(currState, 3), 0.1))\n",
    "        else:\n",
    "            possible_next_states.append((bounds_check(currState, action), 1))\n",
    "        \n",
    "        return possible_next_states\n",
    "        # Sample: [([11, 10], 0.8), ([10, 9], 0.1), ([10, 11], 0.1)] \n",
    "    else:\n",
    "        if absorbing:\n",
    "            return currState\n",
    "        return bounds_check(currState, action)\n",
    "        # Sample [3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(noisy=False):\n",
    "    \"\"\"\n",
    "    Returns the reward which an agent should be given when after it\n",
    "    transitions to a state.\n",
    "    \n",
    "    The method returns a dictionary object, R. The keys are\n",
    "    tuples of all (x,y) coordinates of the environment and the \n",
    "    respective value is the reward an actors receives when it \n",
    "    transitions to that state.\n",
    "    \n",
    "    The flag 'noisy' is useful for Q 1.6 where mines have a 50% \n",
    "    chance of blowing up when the actors is in their direct vicinity.\n",
    "    When 'noise = True' the reward for squares neighboring mines is \n",
    "    calculated as the expected value: \n",
    "    \n",
    "    0.5*(normal value of square, i.e. water) + 0.5*(value of mine square).\n",
    "    \n",
    "    \"\"\"\n",
    "    reward = { \"goal\": 100,\n",
    "               \"mine\": -100,\n",
    "               \"edge\": -1,\n",
    "               \"water\": -1 }\n",
    "    \n",
    "    R = {}\n",
    "    \n",
    "    for state, region in np.ndenumerate(submarine.maze):\n",
    "        r = float()\n",
    "        \n",
    "        if list(state) in submarine.goal_states: # goal\n",
    "            r = reward['goal'] \n",
    "        elif region == 4: # mine\n",
    "            r = reward['mine'] \n",
    "        elif region in (7,8): # sky or edge\n",
    "            r = reward['edge'] \n",
    "        else: # water or unchartered\n",
    "            r = reward['water']\n",
    "          \n",
    "        if noisy:\n",
    "            # Check if mine in neighboring states\n",
    "            mine = False\n",
    "            for direction in range(submarine.num_actions):\n",
    "                neighbor = bounds_check(state, direction)\n",
    "                neighbor_type = maze_array[neighbor[0]][neighbor[1]] \n",
    "                if neighbor_type == 4: # mine\n",
    "                    mine = True\n",
    "            if mine:\n",
    "                r = (r + reward['mine'])*0.5\n",
    "        R[state] = r\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2: Value Iteration\n",
    "\n",
    "\n",
    "Your first algorithmic task will be to apply value iteration to the solution of this navigation task, using the reward and transition functions you have devised in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.8\n",
    "R = calculate_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged at iteration #81\n"
     ]
    }
   ],
   "source": [
    "def val_iteration(R):\n",
    "    V = np.zeros_like(submarine.maze)\n",
    "    V_sum = 0\n",
    "    for i in range(1000):\n",
    "        V_temp = V\n",
    "        for state, region in np.ndenumerate(submarine.maze):\n",
    "            if region == 4 or state in submarine.goal_states: # absorbing state\n",
    "                V_temp[state[0]][state[1]] = 0\n",
    "            else:                \n",
    "                curr_state_val = - float(\"inf\")\n",
    "                for action in range(submarine.num_actions):\n",
    "                    possible_next_states = get_transition(state, action, probabilistic=True)\n",
    "                    total_val = 0\n",
    "                    for next_state in possible_next_states:\n",
    "                        next_state_val = DISCOUNT * V[next_state[0][0]][next_state[0][1]]\n",
    "                        action_val = R[tuple(next_state[0])]\n",
    "                        total_val += next_state[1] * (action_val + next_state_val)\n",
    "                    if total_val > curr_state_val:\n",
    "                        curr_state_val = total_val\n",
    "                V_temp[state[0]][state[1]] = curr_state_val\n",
    "\n",
    "        V = V_temp\n",
    "\n",
    "        if (i % 10) == 0:\n",
    "            if np.abs(V.sum() - V_sum) < 1e-6:\n",
    "                print(f'Value iteration converged at iteration #{i+1:,}')\n",
    "                break\n",
    "            V_sum = V.sum()\n",
    "    return V\n",
    "\n",
    "V = val_iteration(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1715c69438>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17158c5c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(V,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Deterministic Policy\n",
    "\n",
    "Selects the best action based on the highest state-action value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(V, R):\n",
    "    policy = np.zeros_like(V)\n",
    "\n",
    "    for state, region in np.ndenumerate(submarine.maze):\n",
    "        best_action = None\n",
    "        best_action_val = -float(\"inf\")\n",
    "        for action in range(submarine.num_actions):\n",
    "            next_state = get_transition(state, action)\n",
    "            next_state_val = V[next_state[0]][next_state[1]]\n",
    "            action_val = R[tuple(next_state)]\n",
    "            total_val = action_val + next_state_val\n",
    "            if total_val  > best_action_val:\n",
    "                best_action_val = total_val\n",
    "                best_action = action\n",
    "        policy[state[0]][state[1]] = best_action  \n",
    "    return policy\n",
    "\n",
    "policy = extract_policy(V, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a few trajectories\n",
    "\n",
    "Visulise some trajectories from a few different initial points in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vesko/anaconda3/envs/jupyter/lib/python3.6/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAEECAYAAAC1NjdyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111d59f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def viz_trajectory(start_from):\n",
    "    submarine.init_state = start_from\n",
    "    submarine._reset()\n",
    "    state = submarine.init_state\n",
    "    indx = 0\n",
    "    while state not in submarine.goal_states:\n",
    "        submarine._step(policy[tuple(state)])\n",
    "        fig = submarine._render()\n",
    "        state = submarine.state\n",
    "\n",
    "        if indx > 1000:\n",
    "            print(\"Max Iteration Limit Hit!\")\n",
    "            break\n",
    "        indx += 1\n",
    "\n",
    "for i in [[16,2], [4,13], [3,16], [13,2]]:\n",
    "    viz_trajectory(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·'],\n",
       "       ['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '↓', '↓', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '↓', '↓', '↓', 'X', 'X', 'X', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', 'X', 'X', 'X', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '⬤', '·'],\n",
       "       ['·', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', 'X', 'X', 'X', '·'],\n",
       "       ['·', '→', '→', '→', '→', '↑', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', 'X', 'X', 'X', '·'],\n",
       "       ['·', '→', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', 'X', 'X', 'X', '·'],\n",
       "       ['·', '→', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '→', '→', '→', '→', '·'],\n",
       "       ['·', '→', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '→', '→', '↑', '↑', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '→', '→', '↑', '↑', '·'],\n",
       "       ['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·']],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_policy(submarine, policy, show_unchartered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3: Policy Evaluation\n",
    "    \n",
    "Define a function that computes the state-value function $V^\\pi$ on this environment when given a random policy $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(V):\n",
    "    V_new = V\n",
    "    for state, region in np.ndenumerate(submarine.maze):\n",
    "        total_val = 0\n",
    "        if not (region == 4 or state in submarine.goal_states): # absorbing state \n",
    "            action = policy[state]\n",
    "            possible_next_states = get_transition(state, action, probabilistic=True)\n",
    "            \n",
    "            for next_state in possible_next_states:\n",
    "                next_state_val = DISCOUNT * V[next_state[0][0]][next_state[0][1]]\n",
    "                action_val = R[tuple(next_state[0])]\n",
    "                total_val += next_state[1] * (action_val + next_state_val) # next_state[1] = p(next_state|state,action)\n",
    "        V_new[state[0]][state[1]] = total_val     \n",
    "    return V_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.4: Policy Iteration\n",
    "Apply policy iteration to solve the submarine navigation task. \n",
    "- In what ways are the results of policy iteration different from those of value iteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improv(policy, V):\n",
    "    \"\"\" Policy improvement. \"\"\"\n",
    "    for s, region in np.ndenumerate(submarine.maze):\n",
    "        s_best_val = -float(\"inf\")\n",
    "        for act in range(submarine.num_actions): # iterate actions, [0,1,2,3]\n",
    "            possible_next_states = get_transition(s, act, probabilistic=True)\n",
    "            s_act_val = 0\n",
    "            for next_state in possible_next_states:\n",
    "                next_state_val = DISCOUNT * V[next_state[0][0]][next_state[0][1]]\n",
    "                action_val = R[tuple(next_state[0])]\n",
    "                s_act_val += next_state[1] * (action_val + next_state_val)\n",
    "            if s_act_val > s_best_val:\n",
    "                s_best_val = s_act_val\n",
    "                policy[s[0]][s[1]] = act\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.zeros_like(V)\n",
    "V = np.zeros_like(submarine.maze)\n",
    "for i in range(100):\n",
    "    V = policy_eval(V)\n",
    "    policy = policy_improv(policy, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·'],\n",
       "       ['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '↓', '↓', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '↓', '↓', '↓', 'X', 'X', 'X', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', 'X', 'X', 'X', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '⬤', '·'],\n",
       "       ['·', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', 'X', 'X', 'X', '·'],\n",
       "       ['·', '→', '→', '→', '→', '↑', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', 'X', 'X', 'X', '·'],\n",
       "       ['·', '↑', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '↑', '↑', '↑', '←', 'X', 'X', 'X', '·'],\n",
       "       ['·', '↑', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '→', '→', '→', '→', '·'],\n",
       "       ['·', '↑', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '↑', '·'],\n",
       "       ['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·']],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_policy(submarine, policy, show_unchartered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.5: Hyper-parameters\n",
    "\n",
    "For both policy and value iteration:\n",
    "- How does varying the discount factor $\\gamma$ affect the calculated policy? \n",
    "    - Repeat the same experiment with at least three different settings of a discount factor in order to make your argument.\n",
    "- Given the insights from your experiment above, suggest a suitable strategy for setting an appropriate discount rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value Iteration – Hyperparam optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged at iteration #31\n",
      "CPU times: user 405 ms, sys: 7.68 ms, total: 413 ms\n",
      "Wall time: 643 ms\n"
     ]
    }
   ],
   "source": [
    "DISCOUNT = [1e-200, 1e-50, 1e-10, 0.001, 0.05, 0.2, 0.5, 0.8, 0.9, 0.99, 0.999999999]\n",
    "R = calculate_reward()\n",
    "V = val_iteration(R)\n",
    "policy = extract_policy(V, R)\n",
    "# plt.imshow(V,cmap='gray')\n",
    "# draw_policy(submarine, policy, show_unchartered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy Iteration – Hyperparam optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = [1e-200, 1e-50, 1e-10, 0.001, 0.05, 0.2, 0.5, 0.8, 0.9, 0.99, 0.999999999]\n",
    "policy = np.zeros_like(V)\n",
    "V = np.zeros_like(submarine.maze)\n",
    "for i in range(100):\n",
    "    V = policy_eval(V)\n",
    "    policy = policy_improv(policy, V)\n",
    "# plt.imshow(V,cmap='gray')\n",
    "# draw_policy(submarine, policy, show_unchartered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Student Answer **\n",
    "\n",
    "1. After running several experiments above I have the following observations. Setting the discount to a small numbers like 0.1 very quickly interrupts the flow of information in the MDP. If the discount factor is too small (say 1e-50 or even 1e-3), the model cannot learn an optimal policy because the model 'sees' just a few steps beyond its current state, because values of subsequent states are too 'discounted' to be factored in. Thus it may seem like a good idea to set a high discount rate, though always less 1. Research has shown that Dynamic Programming algorithms, as well as SARSA and Q-learning, only converge to an optimal policy only for discounted reward and infinite policy horizon (Sutton & Barto, Reinforcement Learning). Additionally, setting Gamma, the discount factor, to a large value increases the computational cost. A very quick experiment showed an exponential increase of computational time:\n",
    "DISCOUNT = 0.3  -->  0.64 s\n",
    "DISCOUNT = 0.7  -->  0.68 s\n",
    "DISCOUNT = 0.9  -->  1.6  s\n",
    "DISCOUNT = 0.99 --> 10.8  s\n",
    "\n",
    "2. Setting an optimal discount factor is an engineering problem and it depends on the specific configuration of the problem. In a finite MDP, it makes a big difference if the number of states is 10 or 10,000. Additionally, the structure of the MDP matters as well.\n",
    "To illustrate, in the cart pole balancing problem, in which we have to balance a pole indefinitely, we don't need to look too far ahead. We can reward the model when it achhieves balance – which typically happens just a few steps ahead. While other problems, like winning the game of GO require the model to look at many more more steps ahead. The discount factor must always be in the range (0,1). Values like 0.8 often work, though the best value can be found with experimentation only and when the semantics of the problem are taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.6: Noisy trigger\n",
    "\n",
    "The mines now have a 50% chance of blowing up if you are within their direct vicinity (i.e. on their border). \n",
    "\n",
    "How does this affect your policy? \n",
    "\n",
    "Implement this and justify your answer based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged at iteration #71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1120cb198>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCAApAgJAioAAkCIgAKQICAApAgJA6v8AI3JfY67dad8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112072358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "R_noisy = calculate_reward(noisy=True)\n",
    "V_noisy = val_iteration(R_noisy)\n",
    "plt.imshow(V_noisy, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·'],\n",
       "       ['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·'],\n",
       "       ['·', '↑', '↑', '↑', '↑', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↑', '↑', 'X', 'X', 'X', '↑', '↑', '↑', '↑', '↑', '→', '→', '→', '→', '→', '↓', '↓', '·'],\n",
       "       ['·', '↓', '←', 'X', 'X', 'X', '→', '↑', '↑', 'X', 'X', 'X', '↑', '↑', '↑', '→', '→', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '↓', '↓', '←', 'X', 'X', 'X', '↑', 'X', 'X', 'X', '→', '↓', '·'],\n",
       "       ['·', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', 'X', 'X', 'X', '↓', 'X', 'X', 'X', '→', '↓', '·'],\n",
       "       ['·', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↑', '↑', '↑', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '·'],\n",
       "       ['·', '←', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '→', '⬤', '·'],\n",
       "       ['·', '←', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', '←', 'X', 'X', 'X', '·'],\n",
       "       ['·', '←', '←', '←', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '←', 'X', 'X', 'X', '·'],\n",
       "       ['·', '↑', '←', 'X', 'X', 'X', '→', '→', '↑', '↑', '↑', '↑', '↑', '↑', '←', 'X', 'X', 'X', '·'],\n",
       "       ['·', '↑', '←', 'X', 'X', 'X', '→', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '←', '←', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '→', '→', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '←', '←', '←', '·'],\n",
       "       ['·', '↓', '↓', '↓', '↓', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '·'],\n",
       "       ['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·']],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_noisy = extract_policy(V_noisy, R_noisy)\n",
    "draw_policy(submarine, policy_noisy, show_unchartered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "*Remark: This second question need not be implemented in the notebook, in the same way you\n",
    "wrote programs for the first question. Instead, we expect to see a complete problem formulation,\n",
    "with suitable expressions and graphs, and an explanation of the solution procedure in terms of those\n",
    "expressions. The notebook only provides a means by which to explore the question; this question should be completed by hand.*\n",
    "\n",
    "\n",
    "Now consider two additions to the above problem specification, that are common in realistic versions\n",
    "of this problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1) \n",
    "The agent must include within the costs, the ides of navigability - describing the fact that\n",
    "some parts of a given map are easier to travel through than others (typically, due to the ‘diffi-\n",
    "culty’ of performing low-level control with respect to the terrain features found at that depth).\n",
    "One way to model this would be to include a distribution of costs over the given terrain map.\n",
    "By re-writing your complete problem specification, explain how you will incorporate this\n",
    "feature in your modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Student Writeup **\n",
    "\n",
    "One way to include within the cost the idea of navigability is to modify the reward function. We can define a function which captures numerically the the difficulty of navigating the terrain in every square of the environment, assuming the environment is a discrete and finate problem like the submarine challenge from Q 1. Then we can use incrporate the cost of navigability with the original reward function by scaling the reward, in such a way that the water square which has the average cost for navigation is left unchaged and the reward in all other water squares is increased or decreased proportionally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2)\n",
    "The agents in the environment, previously viewed as static obstacles defining where you can\n",
    "and can not traverse, could have their own dynamics. For instance, one of the mines could be\n",
    "an active craft with its own motion policy. Assuming that you can observe the current position\n",
    "of this active craft through a noisy channel (e.g., by interpretation of sonar reflections), pose\n",
    "your - now interactive - motion planning problem in Bayesian terms and explain how your\n",
    "solution strategy might need to be altered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Student Writeup **\n",
    "\n",
    "Assumptions:\n",
    "* We have a noisy $(\\tilde{\\theta_x},\\tilde{\\theta_y})$ observation of the location of the other craft which is sampled from the true unobserved location of the other craft $\\tilde{\\theta_x} \\sim \\mathcal{N}(\\theta_x; \\mu_{\\theta_x}, \\sigma_{\\theta_x}^2)$ and $\\tilde{\\theta_y} \\sim \\mathcal{N}(\\theta_y; \\mu_{\\theta_y}, \\sigma_{\\theta_y}^2) $\n",
    "* We assume that (1) the other craft is definitely in the environment, (2) not on an edge, in the sky or insdie a mine, and (3) our vessel and the other craft can be inside the same square of water.\n",
    "* The location of the other craft depends only on the current observation, and not on prior observations.\n",
    "* Our objective is to avoid interactions with the other craft. An interaction happens when both vessels ends up in the same square. This carries a large amount of negative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a Partially Observable Markov Decision Process, defined by the tuple $ ⟨S, A, T(s,a), R(\\tilde{\\theta}, s'), O(s',a), \\tilde{\\Theta}⟩ $, where:\n",
    "\n",
    "* The state space $S = \\{(x_1,y_1), (x_2,y_2) ... (x_{19},y_{19})\\}, S \\in \\mathbb{R}^{361}$, action space $A = \\{UP, DOWN, LEFT, RIGHT\\}$ and transition function $T$ are defined as in Question 1.\n",
    "* Just like before the submarine transitions from state $s$ to $s'$ according to transition function $T(s,a)$. After each transition, the submarine receives reward $R(s')$ which depends only the state that we transition to.\n",
    "* $O (s',a) $ is an observation function which maps the action we take and our resulting state to a new nosiy observation $O(s',a): S \\times A \\to \\tilde{\\theta}$, such that $(\\tilde{\\theta_x}, \\tilde{\\theta_y}) \\sim \\Theta$\n",
    "* We modify the reward function $R$ to accomodate accomodate for the uncertainty of the location of the other craft. We put a Gaussian probability density anchord in the coordinates of the noisy estimate $(\\tilde{\\theta_x}, \\tilde{\\theta_y})$. The square where the center of mass falls, $(\\tilde{\\theta_x}, \\tilde{\\theta_y})$ receives the full amount of negative award that we choose to model for 'meeting' the other craft. But we also assign radially negative reward in the squares surrounding the best noisy estimate. The magnitude of the negative reward decreases as move further away from the center of mass.\n",
    "* $\\tilde{\\Theta}$ is the set of noisy observations of the other craft. We define $Θ$ to be the discretized set of all 235 possible noisy locations of the craft, $\\tilde{\\Theta} = \\{ \\tilde{\\theta_1}, \\tilde{\\theta_2}, ... \\tilde{\\theta_{235}} \\}, \\tilde{\\Theta} \\in \\mathbb{R}^{235} $ (Note: $235$ is the number of squares of water, including unchartered therritory but excluding the sky, edges and mines, $19*19-17*2-19*2-6*9 = 235$)\n",
    "\n",
    "The probability of transitioning to a new state $s'$ depends on the marginal probability of transitioning to $s'$ given observation $\\tilde{\\theta}$ and on the probability of observation $\\tilde{\\theta}$  occurring in $s$.\n",
    "\n",
    "We can now calculate the expected reward by recursing similarly to the way we implemented value iteration:\n",
    "\n",
    "$V[\\tilde{\\theta},a] = \\mathbb{E}_{s \\sim b}(R(\\theta, s') + \\mathbb{E}_{s',o,a'}(V[\\tilde{\\theta}',a']) )$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
