{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # avoid non-GUI warning for matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from itertools import product as cartesian_product\n",
    "from skimage.draw import circle, circle_perimeter\n",
    "\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(linewidth=250,precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeGenerator(object):\n",
    "    def __init__(self):\n",
    "        self.maze = None\n",
    "    \n",
    "    def init_end_states(self):\n",
    "        \"\"\"Get start and end goals\"\"\"\n",
    "        init_state = [3,1]\n",
    "        goal_states = [[11,17]]\n",
    "        return init_state, goal_states\n",
    "        \n",
    "    def get_maze(self):\n",
    "        return self.maze\n",
    "\n",
    "\n",
    "class SimpleMazeGenerator(MazeGenerator):\n",
    "    def __init__(self, maze):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.maze = maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Region ID's (also related to the region colours, more in utils.py - line 113)\n",
    "regions = {'water':6, 'edge':7, 'sky':8, 'mine':4, 'uncharted':5}\n",
    "\n",
    "env_size = 19\n",
    "maze_array = np.zeros([env_size,env_size]) + regions['water']\n",
    "\n",
    "#Boundary\n",
    "maze_array[:,0] = regions['edge']\n",
    "maze_array[:,-1] = regions['edge']\n",
    "maze_array[-1,:] = regions['edge']\n",
    "maze_array[0:2,:] = regions['sky']\n",
    "\n",
    "#Bombs\n",
    "maze_array[circle(11,3,2)] = regions['mine']\n",
    "maze_array[circle(6,14,2)] = regions['mine']\n",
    "maze_array[circle(15,4,2)] = regions['mine']\n",
    "maze_array[circle(5,10,2)] = regions['mine']\n",
    "maze_array[circle(13,16,2)] = regions['mine']\n",
    "maze_array[circle(4,4,2)] = regions['mine']\n",
    "\n",
    "#Unmapped territory\n",
    "maze_array[circle(12,10,5)] = regions['uncharted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Generate the underwater maze environment\n",
    "maze = SimpleMazeGenerator(maze_array)\n",
    "submarine = MazeEnv(maze, render_trace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10d720198>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIWJgm/QEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a72d6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A view of the environment\n",
    "\n",
    "#reset(), resets the environment back to it's initial conditions and returns a view of it\n",
    "snapshot = submarine._reset() \n",
    "\n",
    "plt.imshow(snapshot, cmap=submarine.cmap, norm=submarine.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of Actions :  4 \n",
      " Action ID: Up, Down, Left, Right =  [0, 1, 2, 3] \n",
      " Goal states:  [[11, 17]] \n",
      " Maze size:  (19, 19) \n",
      " List of all states in the environment:  [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4)] ...\n"
     ]
    }
   ],
   "source": [
    "# Some available information from the environment\n",
    "print(  \" Number of Actions : \", submarine.num_actions,\n",
    "      \"\\n Action ID: Up, Down, Left, Right = \",submarine.all_actions,\n",
    "      \"\\n Goal states: \", submarine.goal_states,\n",
    "      \"\\n Maze size: \", submarine.maze_size,\n",
    "      \"\\n List of all states in the environment: \", submarine.valid_states[0:5], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10d831b70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYmQ+Kd0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a72d240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unlike the traditional gym environment, each call to the step() function executes an \n",
    "# action and takes a step through the dynamics of the environment.\n",
    "# The resulting environmental observation is returned. \n",
    "# Note: This function updates the state of the executing agent.\n",
    "\n",
    "\n",
    "submarine._step(1)\n",
    "submarine._step(1)\n",
    "submarine._step(1)\n",
    "snapshot = submarine._step(1)\n",
    "plt.imshow(snapshot, cmap=submarine.cmap, norm=submarine.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10d911828>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYvj9IecAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d7a4e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submarine._step(3)\n",
    "submarine._step(3)\n",
    "snapshot = submarine._step(3)\n",
    "plt.imshow(snapshot, cmap=submarine.cmap, norm=submarine.norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _next_state(current_state,action) takes a state and an action as an input and \n",
    "# returns the resulting state.\n",
    "# Note: This function does not update the state of the executing agent.\n",
    "submarine._next_state((0,0),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CourseWork\n",
    "\n",
    "You are to design an agent that will be in charge of the navigation of a submarine, through a\n",
    "minefield, towards a goal.\n",
    "\n",
    "Navigating through uncharted territory can be dangerous, which we will model by applying some damage to the submarine. Additionally, there is some uncertainty in the execution of actions when moving through uncharted territory. Currents are strong and there is a 20% chance that the submarine will diverge away from its path during execution. For example, given that our current state is (12,10) within the uncharted territory, and we execute a down action; there is an 80% chance it will go down to (13,10), but also a 10% chance that it will go left to (12,9), and another 10% chance it will go right to (12,11).\n",
    "\n",
    "\n",
    "Hitting a mine will lead to the destruction of the submarine and boundary regions marked **edge** or **sky** cannot be traversed.  Your task is to develop a strategy for this agent (plotted as the *blue* dot) to safely navigate towards the goal (plotted as the *green* dot).\n",
    "\n",
    "You are to do this using the principles of Dynamic Programming. In particular, in this exercise,\n",
    "you will implement the methods of Policy and Value Iteration. For all of these questions, please use\n",
    "the provided visualisation functions to plot your policies and their associated trajectories. Include\n",
    "concise comments within the notebook to highlight any interesting observations to support your\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.1 : Reward and Transition Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define suitable reward and transition functions for the submarine environment. This is a key task for you as the\n",
    "designer of this agent. \n",
    "\n",
    "The reward function specifies a scalar reward for every state in the environment (in accordance to the coursework description above) and the transition function determines the next *probable* state, $s'$, reached when an action $a$ is taken from state $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2341)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "regions = {'water':6, 'edge':7, 'sky':8, 'mine':4, 'uncharted':5}\n",
    "maze_array[y,x]\n",
    "Action ID: Up, Down, Left, Right =  [0, 1, 2, 3] \n",
    "\n",
    "0 Up\n",
    "1 Down\n",
    "2 Left\n",
    "3 Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition(currState, action, probabilistic=False):\n",
    "    \n",
    "    def bounds_check(direction): \n",
    "        \"\"\"\n",
    "        Given a direction of movement, this method returns the transition state \n",
    "        as determined by _next_state(), unless the result falls outside the \n",
    "        bounds of the environment. In that case, the method returns currState.\n",
    "        \n",
    "        \"\"\"\n",
    "        goalState = submarine._next_state(currState, direction)\n",
    "        if \\\n",
    "        (goalState[0] < 0) or \\\n",
    "        (goalState[1] < 0) or \\\n",
    "        (goalState[0] > env_size - 1) or \\\n",
    "        (goalState[1] > env_size - 1):\n",
    "            return currState\n",
    "        else:\n",
    "            return goalState\n",
    "    \n",
    "    # region type of current state\n",
    "    currRegion = maze_array[currState[0]][currState[1]] \n",
    "    \n",
    "    if probabilistic:\n",
    "        possible_next_states = []\n",
    "        if currRegion == 5: # unchartered waters\n",
    "            possible_next_states.append((bounds_check(action), 0.8))\n",
    "            if action in (2,3):\n",
    "                possible_next_states.append((bounds_check(0), 0.1))\n",
    "                possible_next_states.append((bounds_check(1), 0.1))\n",
    "            elif action in (0,1):\n",
    "                possible_next_states.append((bounds_check(2), 0.1))\n",
    "                possible_next_states.append((bounds_check(3), 0.1))\n",
    "        else:\n",
    "            possible_next_states.append((bounds_check(action), 1))\n",
    "        \n",
    "        return possible_next_states\n",
    "        # Sample: [([11, 10], 0.8), ([10, 9], 0.1), ([10, 11], 0.1)] \n",
    "    else:\n",
    "        return bounds_check(action)\n",
    "        # Sample [3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(nextState):\n",
    "    \"\"\"\n",
    "    Returns reward based on the state \n",
    "    which the submarine transitions to.\n",
    "    \"\"\"\n",
    "    \n",
    "    nextRegion = maze_array[nextState[0]][nextState[1]] \n",
    "    \n",
    "    if list(nextState) in submarine.goal_states:\n",
    "        return 20 # goal\n",
    "    elif nextRegion == 4: \n",
    "        return -20 # mine\n",
    "    elif nextRegion in (7,8): \n",
    "        return -10 # sky or edge\n",
    "    else:\n",
    "        return -1 # water or unchartered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.2: Value Iteration\n",
    "\n",
    "\n",
    "Your first algorithmic task will be to apply value iteration to the solution of this navigation task, using the reward and transition functions you have devised in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged at iteration #71\n"
     ]
    }
   ],
   "source": [
    "V = np.zeros_like(submarine.maze)\n",
    "V_sum = 0\n",
    "for i in range(1000):\n",
    "    V_temp = V\n",
    "    for state, region in np.ndenumerate(submarine.maze):\n",
    "        curr_state_val = - float(\"inf\")\n",
    "        for action in range(submarine.num_actions):\n",
    "            possible_next_states = get_transition(state, action, probabilistic=True)\n",
    "            total_val = 0\n",
    "            for next_state in possible_next_states:\n",
    "                next_state_val = DISCOUNT * V[next_state[0][0]][next_state[0][1]]\n",
    "                action_val = get_reward(next_state[0])\n",
    "                total_val += next_state[1] * (action_val + next_state_val)\n",
    "            if total_val > curr_state_val:\n",
    "                curr_state_val = total_val\n",
    "        V_temp[state[0]][state[1]] = curr_state_val\n",
    "    \n",
    "    V = V_temp\n",
    "        \n",
    "    if (i % 10) == 0:\n",
    "        if np.abs(V.sum() - V_sum) < 1e-6:\n",
    "            print(f'Value iteration converged at iteration #{i+1:,}')\n",
    "            break\n",
    "        V_sum = V.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17141004e0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAfC8wI3BJOzxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x171416c748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(V,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Deterministic Policy\n",
    "\n",
    "Selects the best action based on the highest state-action value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.zeros_like(V)\n",
    "\n",
    "for state, region in np.ndenumerate(submarine.maze):\n",
    "    best_action = None\n",
    "    best_action_val = -float(\"inf\")\n",
    "    for action in range(submarine.num_actions):\n",
    "        next_state = get_transition(state, action)\n",
    "        next_state_val = V[next_state[0]][next_state[1]]\n",
    "        action_val = get_reward(next_state)\n",
    "        total_val = action_val + next_state_val\n",
    "        if total_val  > best_action_val:\n",
    "            best_action_val = total_val\n",
    "            best_action = action\n",
    "    policy[state[0]][state[1]] = best_action  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test a few trajectories\n",
    "\n",
    "Visulise some trajectories from a few different initial points in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vesko/anaconda3/envs/jupyter/lib/python3.6/site-packages/matplotlib/figure.py:418: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1713e87828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def viz_trajectory(start_from):\n",
    "    submarine.init_state = start_from\n",
    "    submarine._reset()\n",
    "    state = submarine.init_state\n",
    "    indx = 0\n",
    "    while state not in submarine.goal_states:\n",
    "        submarine._step(policy[tuple(state)])\n",
    "        fig = submarine._render()\n",
    "        state = submarine.state\n",
    "\n",
    "        if indx > 1000:\n",
    "            print(\"Max Iteration Limit Hit!\")\n",
    "            break\n",
    "        indx += 1\n",
    "\n",
    "for i in [[16,2], [4,13], [3,16], [13,2]]:\n",
    "    viz_trajectory(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submarine.init_state = [16,2] #[3,16] #[13,2] [4,13]\n",
    "# submarine._reset()\n",
    "# state = submarine.init_state\n",
    "# indx = 0\n",
    "# while state not in submarine.goal_states:\n",
    "#     submarine._step(policy[tuple(state)])\n",
    "#     fig = submarine._render()\n",
    "#     state = submarine.state\n",
    "    \n",
    "#     if indx > 1000:\n",
    "#         print(\"Max Iteration Limit Hit!\")\n",
    "#         break\n",
    "#     indx += 1\n",
    "# anime = submarine._get_video(interval=200, gif_path='value_iter.mp4').to_html5_video()\n",
    "# HTML(anime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trajectories can also be visualied via the simulate function \n",
    "# # (note: function buggy on some systems; needs to be run thrice. )\n",
    "# submarine.policy = policy\n",
    "# HTML(simulate(submarine, [3,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓'],\n",
       "       ['↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓'],\n",
       "       ['→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '←'],\n",
       "       ['→', '↓', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '↓', '↓', '←'],\n",
       "       ['→', '↓', '↓', 'X', 'X', 'X', '↓', '↓', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '↓', '↓', '←'],\n",
       "       ['→', '↓', '↓', 'X', 'X', 'X', '↓', '↓', '↓', 'X', 'X', 'X', '↓', 'X', 'X', 'X', '↓', '↓', '←'],\n",
       "       ['→', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', 'X', 'X', 'X', '↓', 'X', 'X', 'X', '↓', '↓', '←'],\n",
       "       ['→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', 'X', 'X', 'X', '↓', '↓', '←'],\n",
       "       ['→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '←'],\n",
       "       ['→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '←'],\n",
       "       ['→', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '←'],\n",
       "       ['→', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '⬤', '←'],\n",
       "       ['→', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', 'X', 'X', 'X', '↑'],\n",
       "       ['→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', 'X', 'X', 'X', '↑'],\n",
       "       ['→', '→', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', 'X', 'X', 'X', '↑'],\n",
       "       ['→', '→', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '↑', '←', '←', '←', '←'],\n",
       "       ['→', '→', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '←'],\n",
       "       ['→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '↑', '←'],\n",
       "       ['→', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑']],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_policy(submarine, policy, show_sky_edge=False, show_unchartered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·'],\n",
       "       ['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '↓', '↓', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '↓', '↓', '↓', 'X', 'X', 'X', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', 'X', 'X', 'X', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '⬤', '·'],\n",
       "       ['·', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', 'X', 'X', 'X', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', 'X', 'X', 'X', '·'],\n",
       "       ['·', '→', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', 'X', 'X', 'X', '·'],\n",
       "       ['·', '→', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '↑', '←', '←', '←', '·'],\n",
       "       ['·', '→', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '↑', '·'],\n",
       "       ['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·']],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_policy(submarine, policy, show_unchartered=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.3: Policy Evaluation\n",
    "    \n",
    "Define a function that computes the state-value function $V^\\pi$ on this environment when given a random policy $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(V):\n",
    "    V_new = V\n",
    "    for state, region in np.ndenumerate(submarine.maze):\n",
    "        action = policy[state]\n",
    "        possible_next_states = get_transition(state, action, probabilistic=True)\n",
    "        total_val = 0\n",
    "        for next_state in possible_next_states:\n",
    "            next_state_val = DISCOUNT * V[next_state[0][0]][next_state[0][1]]\n",
    "            action_val = get_reward(next_state[0])\n",
    "            total_val += next_state[1] * (action_val + next_state_val) # next_state[1] = p(next_state|state,action)\n",
    "        V_new[state[0]][state[1]] = total_val     \n",
    "    return V_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.4: Policy Iteration\n",
    "Apply policy iteration to solve the submarine navigation task. \n",
    "- In what ways are the results of policy iteration different from those of value iteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improv(policy, V):\n",
    "    \"\"\" Policy improvement. \"\"\"\n",
    "    for s, region in np.ndenumerate(submarine.maze):\n",
    "        s_best_val = -float(\"inf\")\n",
    "        for act in range(submarine.num_actions): # iterate actions, [0,1,2,3]\n",
    "            possible_next_states = get_transition(s, act, probabilistic=True)\n",
    "            s_act_val = 0\n",
    "            for next_state in possible_next_states:\n",
    "                next_state_val = DISCOUNT * V[next_state[0][0]][next_state[0][1]]\n",
    "                action_val = get_reward(next_state[0])\n",
    "                s_act_val += next_state[1] * (action_val + next_state_val)\n",
    "            if s_act_val > s_best_val:\n",
    "                s_best_val = s_act_val\n",
    "                policy[s[0]][s[1]] = act\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.zeros_like(V)\n",
    "V = np.zeros_like(submarine.maze)\n",
    "for i in range(100):\n",
    "    V = policy_eval(V)\n",
    "    policy = policy_improv(policy, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·'],\n",
       "       ['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '↓', '↓', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', 'X', 'X', 'X', '↓', '↓', '↓', 'X', 'X', 'X', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '↓', '↓', '↓', '↓', '↓', '↓', '↓', '↓', 'X', 'X', 'X', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', 'X', 'X', 'X', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '↓', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↓', '↓', '↓', '·'],\n",
       "       ['·', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '⬤', '·'],\n",
       "       ['·', '↓', 'X', 'X', 'X', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', 'X', 'X', 'X', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', 'X', 'X', 'X', '·'],\n",
       "       ['·', '↑', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', 'X', 'X', 'X', '·'],\n",
       "       ['·', '↑', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '↑', '←', '←', '←', '·'],\n",
       "       ['·', '↑', '↑', 'X', 'X', 'X', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '↑', '↑', '↑', '·'],\n",
       "       ['·', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '→', '↑', '↑', '↑', '↑', '↑', '·'],\n",
       "       ['·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·', '·']],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_policy(submarine, policy, show_unchartered=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.5: Hyper-parameters\n",
    "\n",
    "For both policy and value iteration:\n",
    "- How does varying the discount factor $\\gamma$ affect the calculated policy? \n",
    "    - Repeat the same experiment with at least three different settings of a discount factor in order to make your argument.\n",
    "- Given the insights from your experiment above, suggest a suitable strategy for setting an appropriate discount rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1.6: Noisy trigger\n",
    "\n",
    "The mines now have a 50% chance of blowing up if you are within their direct vicinity (i.e. on their border). \n",
    "\n",
    "How does this affect your policy? \n",
    "\n",
    "Implement this and justify your answer based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:\n",
    "*Remark: This second question need not be implemented in the notebook, in the same way you\n",
    "wrote programs for the first question. Instead, we expect to see a complete problem formulation,\n",
    "with suitable expressions and graphs, and an explanation of the solution procedure in terms of those\n",
    "expressions. The notebook only provides a means by which to explore the question; this question should be completed by hand.*\n",
    "\n",
    "\n",
    "Now consider two additions to the above problem specification, that are common in realistic versions\n",
    "of this problem:\n",
    "1. The agent must include within the costs, the ides of navigability - describing the fact that\n",
    "some parts of a given map are easier to travel through than others (typically, due to the ‘diffi-\n",
    "culty’ of performing low-level control with respect to the terrain features found at that depth).\n",
    "One way to model this would be to include a distribution of costs over the given terrain map.\n",
    "By re-writing your complete problem specification, explain how you will incorporate this\n",
    "feature in your modelling.\n",
    "2. The agents in the environment, previously viewed as static obstacles defining where you can\n",
    "and can not traverse, could have their own dynamics. For instance, one of the mines could be\n",
    "an active craft with its own motion policy. Assuming that you can observe the current position\n",
    "of this active craft through a noisy channel (e.g., by interpretation of sonar reflections), pose\n",
    "your - now interactive - motion planning problem in Bayesian terms and explain how your\n",
    "solution strategy might need to be altered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
